testmode: false # If you want to test with a small amount of data, please set it to true.
model_name: "nii/llama-2-13b-exp4-instruct" # will be used in Table

wandb:
  entity: "geniac"
  project: "eval-geniac-private"
  run_name: "nii/llama-2-13b-exp4-instruct" # this run_name will be used as the name of run in leaderboard. Can be changed later

# Tasks to run
run_llm_jp_eval_ja_0_shot: true
run_llm_jp_eval_ja_few_shots: true
run_llm_jp_eval_en_0_shot: true
run_llm_jp_eval_en_few_shots: true
run_mt_bench_ja: true
run_mt_bench_en: true

model:
  api: false # if you don't use api, please set "api" as "false". If you use api, please select from "openai", "anthoropic", "google", "cohere", "mistral", "amazon_bedrock"
  use_wandb_artifacts: true # if you user wandb artifacts, please set true.
  artifacts_path: "nii-geniac/Llama-2-175B-hf-checkpoints/Llama-2-13b-exp4-instruct:v0"  # if you user wandb artifacts, please paste the link. if not, please leave it as "".
  pretrained_model_name_or_path: "" #If you use openai api, put the name of model
  device_map: "auto"
  load_in_8bit: false
  load_in_4bit: false

# for llm-jp-eval
llm_jp_eval:
  max_seq_length: 4096
  target_dataset: "all" # {all, jamp, janli, jcommonsenseqa, jemhopqa, jnli, jsem, jsick, jsquad, jsts, niilc, chabsa, mmlu_en}
  ja_num_shots: 4 # if run_llm_jp_eval_ja_few_shots is true, please set the num of few shots. Default is 4
  en_num_shots: 4 # run_llm_jp_eval_en_few_shots is true, please set the num of few shots. Default is 4
  torch_dtype: "bf16" # {fp16, bf16, fp32}
  # Items that do not need to be changed unless specifically intended.
  dataset_artifact: "wandb-japan/llm-leaderboard/jaster:v11"
  dataset_dir: "/jaster/1.2.6/evaluation/test"
  ja: 
    custom_prompt_template: "以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\n### 指示:\n{instruction}\n### 入力:\n{input}\n### 応答:\n"
    custom_fewshots_template: "\n### 入力:\n{input}\n\n### 応答:\n{output}\n"
  en: 
    custom_prompt_template: "以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\n### 指示:\n{instruction}\n### 入力:\n{input}\n### 応答:\n"
    custom_fewshots_template: "\n### 入力:\n{input}\n\n### 応答:\n{output}\n"
    # if you use this, please include {instruction} and {input}. If you use few shots, please include {few_shots} additionally.
    # example of prompt template with fewshots
    # "以下はタスクを説明する指示と、追加の背景情報を提供する入力の組み合わせです。要求を適切に満たす回答を書いてください。\n### 指示：\n{instruction}\n{few_shots}\n### 入力：\n{input}\n### 回答：\n"
    # example of prompt template without fewshots
    # "以下はタスクを説明する指示と、追加の背景情報を提供する入力の組み合わせです。要求を適切に満たす回答を書いてください。\n### 指示：\n{instruction}\n### 入力：\n{input}\n### 回答：\n"
    # example of LLama2 format; plase change tokenizer.bos_token
    # "<tokenizer.bos_token>[INST] <<SYS>>\n あなたは誠実で優秀な日本人のアシスタントです。 \n<</SYS>>\n\n {instruction} \n\n {input} [/INST]"
    # Please include {input} and {output} as variables
    # example of fewshots template
    # "\n### 入力：\n{input}\n### 回答：\n{output}"


# for mtbench
mtbench:
  model_id: "nii--llama-2-13b-exp4-instruct" # cannot use '<', '>', ':', '"', '/', '\\', '|', '?', '*', '.'  
  max_new_token: 1024
  num_gpus_per_model: 8
  num_gpus_total: 8
  max_gpu_memory: null
  parallel: 80
  dtype: bfloat16 # None or float32 or float16 or bfloat16
  use_azure: false # if you use azure openai service for evaluation, set true
  # for conv template # added
  custom_conv_template: true
  # the following variables will be used when custom_conv_template is set as true
  conv_name: "custom"
  conv_sep: "\n\n### "
  conv_stop_token_ids: "[2]"
  conv_stop_str: "###"
  conv_role_message_separator: ":\n"
  conv_role_only_separator: ":\n"
  ja:
    conv_system_message: "以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。"
    conv_roles: "('指示', '応答')"
  en:
    conv_system_message: "以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。"
    conv_roles: "('指示', '応答')"
  dataset: # Items that do not need to be changed unless specifically intended.
    ja:
      question_artifacts_path: "geniac/eval-geniac-private/mtbench_ja_question:v0" 
      test_question_artifacts_path: "geniac/eval-geniac-private/mtbench_ja_question_small_for_test:v0"
      referenceanswer_artifacts_path: "geniac/eval-geniac-private/mtbench_ja_referenceanswer:v0"
      test_referenceanswer_artifacts_path: "geniac/eval-geniac-private/mtbench_ja_referenceanswer_small_for_test:v0"
      judge_prompt_artifacts_path: "geniac/eval-geniac-private/mtbench_ja_prompt:v0"
      bench_name: "mt_bench_ja"
    en:
      question_artifacts_path: "geniac/eval-geniac-private/mtbench_en_question:v0"
      test_question_artifacts_path: "geniac/eval-geniac-private/mtbench_en_question_small_for_test:v0"
      referenceanswer_artifacts_path: "geniac/eval-geniac-private/mtbench_en_referenceanswer:v0" 
      test_referenceanswer_artifacts_path: "geniac/eval-geniac-private/mtbench_en_referenceanswer_small_for_test:v0"
      judge_prompt_artifacts_path: "geniac/eval-geniac-private/mtbench_en_prompt:v0"
      bench_name: "mt_bench_en"


#==================================================================
# Items that do not need to be changed unless specifically intended.
#==================================================================
github_version: g-eval-v1.0 #for recording
