wandb:
  run_name: "Qwen/Qwen3-14B:reasoning" # use run_name defined above

# if you don't use api, please set "api" as "false"
# if you use api, please select from "openai", "anthoropic", "google", "cohere", "vllm"
api: vllm-docker
num_gpus: 1
batch_size: 32 # vllmは256, apiは32を推奨
# inference_interval: 1 # seconds

model:
  use_wandb_artifacts: false
  pretrained_model_name_or_path: "Qwen/Qwen3-14B" #if you use openai api, put the name of model
  bfcl_model_id: "Qwen/Qwen3-14B-FC"
  size_category: "10B≤ <30B"
  size: 1
  release_date: "4/28/2025"

vllm: # vLLM server-side launch configuration
  vllm_tag: v0.8.5
  lifecycle: always_on
  reasoning_parser: qwen3
  gpu_memory_utilization: 0.95
  enable_auto_tool_choice: true
  tool_call_parser: hermes
  extra_args:
    - '--override_generation_config={"temperature":0.6,"top_p":0.95}'

generator:
  temperature: 0.6
  top_p: 0.95
  extra_body:
    chat_template_kwargs:
      enable_thinking: true

jaster:
  override_max_tokens: 16384

jbbq:
  generator_config:
    max_tokens: 16384

jtruthfulqa:
  generator_config:
    max_tokens: 16384

swebench:
  max_tokens: 16384

mtbench:
  max_new_token: 16384
  temperature_override:
    writing: 0.6
    roleplay: 0.6
    extraction: 0.6
    math: 0.6
    coding: 0.6
    reasoning: 0.6
    stem: 0.6
    humanities: 0.6

hallulens:
  generator_config:
    max_tokens: 16384
    temperature: 0.6
    top_p: 0.95

hle:
  max_completion_tokens: 16384

arc_agi_2:
  max_output_tokens: 16384

bfcl:
  temperature: 0.6
  max_tokens: 16384

testmode: false
