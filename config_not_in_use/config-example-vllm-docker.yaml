wandb:
  run_name: "meta-llama/Llama-3.2-1B-Instruct"

api: "vllm-docker"
base_url: "http://vllm:8000/v1"  # Dockerネットワーク内のサービス名

model:
  pretrained_model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  dtype: 'float16'
  max_model_len: 3000
  trust_remote_code: true

# vLLM server configuration
vllm:
  extra_args:
    - --max-model-len=3000
    - --trust-remote-code

# Number of GPUs for tensor parallelism
num_gpus: 1 