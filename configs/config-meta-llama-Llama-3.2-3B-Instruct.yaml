wandb:
  run_name: meta-llama/Llama-3.2-3B-Instruct

api: vllm-docker
base_url: http://vllm:8000/v1
batch_size: 32


model:
  use_wandb_artifacts: false
  pretrained_model_name_or_path: meta-llama/Llama-3.2-3B-Instruct #if you use openai api, put the name of model
  bfcl_model_id: unified-oss-jsonschema
  size_category: Small (<10B)
  size: 3210000000
  release_date: 9/25/2024


generator:
  extra_body:
    chat_template_kwargs:
        enable_thinking: false
  max_tokens: 40000 
  temperature: 0.1
  top_p: 1

vllm:
    disable_triton_mma: false
    dtype: null
    gpu_memory_utilization: 0.8
    lifecycle: always_on
    max_model_len: 40000
    #reasoning_parser: 
    enable_auto_tool_choice: true
    tool_call_parser: llama3_json
    trust_remote_code: true
    vllm_tag: v0.8.5

# Number of GPUs for tensor parallelism
num_gpus: 1 
