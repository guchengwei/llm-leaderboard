wandb:
  log: True
  entity: "wandb-korea"
  project: "llm-leaderboard-experiment"
  run_name: ""

github_version: v2.0.0 #for recording

testmode: false

# if you don't use api, please set "api" as "false"
# if you use api, please select from "openai", "anthropic", "google", "cohere", "mistral", "amazon_bedrock"
api: false

model:
  use_wandb_artifacts: false
  artifacts_path: ""
  pretrained_model_name_or_path: "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct" #if you use openai api, put the name of model
  trust_remote_code: true
  device_map: "auto"
  load_in_8bit: false
  load_in_4bit: false

generator:
  top_p: 1.0
  top_k: 0
  temperature: 0.1
  repetition_penalty: 1.0

tokenizer:
  use_wandb_artifacts: false
  artifacts_path: ""
  pretrained_model_name_or_path: "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
  use_fast: true

# for llm-kr-eval
strict: false
max_seq_length: 2048
dataset_artifact: "wandb-korea/llm-leaderboard-experiment/kaster:v2" #if you use artifacts, please fill here (if not, fill null)
dataset_dir: "/test"
target_dataset: "all" 
log_dir: "./logs"
torch_dtype: "bf16" # {fp16, bf16, fp32}
chat_template_path: "/home/llm-leaderboard/chat_templates/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct_v1.jinja"
# custom_prompt_template: "[|system|][|endofturn|]\n\
#     [|user|]\n다음은 작업을 설명하는 지침과 추가 배경 정보를 제공하는 입력의 조합입니다. 요청을 적절하게 충족하는 답변을 작성하십시오.\
#     \ \n 지시:\n{instruction}\n\n입력:\n{input}\n답변:\n[|assistant|]"
# if you use this, please include {instruction} and {input}. If you use few shots, please include {few_shots} additionally.
# example of prompt template with fewshots
# "다음은 작업을 설명하는 지침과 추가 배경 정보를 제공하는 입력의 조합입니다. 요청을 적절하게 충족하는 답변을 작성하십시오. \n### 지시:\n{instruction}\n{few_shots}\n### 입력:\n{input}\n### 답변:\n"
# example of prompt template without fewshots
# "다음은 작업을 설명하는 지침과 추가 배경 정보를 제공하는 입력의 조합입니다. 요청을 적절하게 충족하는 답변을 작성하십시오. \n### 지시:\n{instruction}\n### 입력:\n{input}\n### 답변:\n"
# example of LLama2 format; plase change tokenizer.bos_token
# "<tokenizer.bos_token>[INST] <<SYS>>\n 당신은 정직하고 우수한 한국인 조수입니다. \n<</SYS>>\n\n {instruction} \n\n {input} [/INST]"

custom_fewshots_template: null
# Please include {input} and {output} as variables
# example of fewshots template
# "\n### 입력:\n{input}\n### 답변:\n{output}"

metainfo:
  basemodel_name: "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
  model_type: "open llm" # {open llm, commercial api}
  instruction_tuning_method: "None" # {"None", "Full", "LoRA", ...}
  instruction_tuning_data: ["None"] # {"None", "kaster", ...}
  num_few_shots: 0
  llm-jp-eval-version: "1.1.0"

# for mtbench
mtbench:
  question_artifacts_path: 'wandb-korea/korean-llm-leaderboard/mtbench_ko_question:v0' # if testmode is true, small dataset will be used
  referenceanswer_artifacts_path: 'wandb-korea/korean-llm-leaderboard/mtbench_ko_referenceanswer:v0' # if testmode is true, small dataset will be used
  small_question_artifacts_path: wandb-korea/korean-llm-leaderboard/mtbench_ko_question_small_for_test:v0
  small_referenceanswer_artifacts_path: wandb-korea/korean-llm-leaderboard/mtbench_ko_referenceanswer_small_for_test:v0
  judge_prompt_artifacts_path: 'wandb-korea/korean-llm-leaderboard/mtbench_ko_prompt:v0'
  bench_name: 'korean_mt_bench'
  model_id: null # cannot use '<', '>', ':', '"', '/', '\\', '|', '?', '*', '.'
  question_begin: null 
  question_end: null 
  max_new_token: 1024
  num_choices: 1
  num_gpus_per_model: 1
  num_gpus_total: 1
  max_gpu_memory: null
  dtype: bfloat16 # None or float32 or float16 or bfloat16
  # for gen_judgment
  judge_model: 'gpt-4'
  mode: 'single'
  baseline_model: null 
  parallel: 1
  first_n: null
  # for conv template # added
  custom_conv_template: false 
  # the following variables will be used when custom_conv_template is set as true
  conv_name: "custom"
  conv_system_message: "다음은 작업을 설명하는 지침과 컨텍스트 입력의 조합입니다. 요구를 적절하게 만족시키는 응답을 적으십시오."
  conv_roles: "('지시', '응답')"
  conv_sep: "\n\n### "
  conv_stop_token_ids: "[2]"
  conv_stop_str: "###"
  conv_role_message_separator: ": \n"
  conv_role_only_separator: ": \n"
