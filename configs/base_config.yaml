wandb:
  entity: 'llm-leaderboard'
  project: 'nejumi-leaderboard4-dev'
  #run: please set up run name in a model-base config

github_version: v3.0.0 #for recording

testmode: true
inference_interval: 0 # seconds

run:
  jaster: true
  jmmlu_robustness: true # if this is set as true, jaster should set as true
  mtbench: false
  jbbq: false
  toxicity: false
  jtruthfulqa: false
  swebench: false
  hallulens: false
  aggregate: false

model:
  artifact_path: null
  max_model_len: 3000
  chat_template: null
  dtype: 'float16'
  trust_remote_code: true
  device_map: 'auto'
  load_in_8bit: false
  load_in_4bit: false

generator:
  top_p: 1.0
  temperature: 0.1
  max_tokens: 128

num_few_shots: 2

jaster:
  message_intro: '以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。'
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/jaster:production'
  dataset_dir: 'jaster'

jbbq:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4-private/jbbq:production'
  dataset_dir: 'jbbq'
  language: 'ja'

toxicity:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4-private/toxicity_dataset_full:production'
  judge_prompts_path: 'llm-leaderboard/nejumi-leaderboard4-private/toxicity_judge_prompts:production'
  max_workers: 5
  judge_model: 'gpt-4o-2024-05-13'

jtruthfulqa:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4/jtruthfulqa_dataset:production'  # JTruthfulQAデータセットのアーティファクトパス
  roberta_model_name: 'nlp-waseda/roberta_jtruthfulqa'  # 評価に使用するRoBERTaモデル名

swebench:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/swebench_verified_official:production'  # SWE-Bench Verifiedデータセットのアーティファクトパス
  dataset_dir: 'swebench_verified_official'
  max_samples: 100  # 全500件は時間がかかるため制限（フルセットの場合は500を指定）
  max_tokens: 2048  # SWE-Benchのパッチ生成に必要なトークン数
  max_workers: 4  # 並列実行ワーカー数
  evaluation_method: 'official'  # 'official' or 'docker' - 公式パッケージまたは独自Docker実装

mtbench:
  temperature_override:
    writing: 0.7
    roleplay: 0.7
    extraction: 0.0
    math: 0.0
    coding: 0.0
    reasoning: 0.0
    stem: 0.1
    humanities': 0.1
  question_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_question:production' # if testmode is true, small dataset will be used
  question_artifacts_path_test: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_question_small_for_test:production'
  referenceanswer_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_referenceanswer:production' # if testmode is true, small dataset will be used
  referenceanswer_artifacts_path_test: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_referenceanswer_small_for_test:production'
  judge_prompt_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_prompt:production' 
  bench_name: 'japanese_mt_bench'
  model_id: null # cannot use '<', '>', ':', ''', '/', '\\', '|', '?', '*', '.'
  question_begin: null 
  question_end: null 
  max_new_token: 1024
  num_choices: 1
  num_gpus_per_model: 1
  num_gpus_total: 1
  max_gpu_memory: null
  dtype: bfloat16 # None or float32 or float16 or bfloat16
  # for gen_judgment
  judge_model: 'gpt-4o-2024-05-13'
  mode: 'single'
  baseline_model: null 
  parallel: 80
  first_n: null

hallulens:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/hallulens:production'
  judge_model: 'gpt-4o'

sample_dataset:
  artifacts_path: 'your artifact path here'
  # add necessary configration here

