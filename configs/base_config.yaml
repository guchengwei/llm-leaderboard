wandb:
  entity: 'llm-leaderboard'
  project: 'nejumi-leaderboard4-dev'
  #run: please set up run name in a model-base config

github_version: v3.0.0 #for recording

testmode: true
inference_interval: 0 # seconds

run:
  jaster: true
  jmmlu_robustness: true # if this is set as true, jaster should set as true
  mtbench: true
  jbbq: true
  toxicity: true
  jtruthfulqa: true
  hle: true
  swebench: true
  bfcl: true
  hallulens: true
  arc_agi_2: true
  m_ifeval: true
  aggregate: true

model:
  artifact_path: null
  ### deprecated: for in-process vllm launch ###
  max_model_len: 3000
  chat_template: null
  dtype: 'float16'
  trust_remote_code: true
  device_map: 'auto'
  load_in_8bit: false
  load_in_4bit: false
  ### deprecated ###

vllm: # vLLM server-side launch configuration
  extra_args: []
  # vLLM docker image tag (default). Override in each model config with `vllm_tag:` or `vllm_docker_image:`
  vllm_tag: latest
  # Volta/Turing など Ampere 未満の GPU で vLLM が Triton MMA カーネルを使ってクラッシュする場合に true に設定
  disable_triton_mma: false
  # vLLMコンテナのライフサイクル管理: auto | always_on | stop_restart
  lifecycle: auto
  # モデルのデータ型: bfloat16 (推奨), float16, half, auto
  dtype: bfloat16
  # example
  # extra_args:
  #   - --dtype=bfloat16
  #   - --max_model_len=3000
  #   - '--override_generation_config={"temperature":0.6,"top_p":0.95}' # do not include space
  #   - --trust_remote_code
  #   - --reasoning-parser=deepseek_r1

generator: # LLM client default configuration: to be overridden by each benchmark
  top_p: 1.0
  temperature: 0.1
  max_tokens: 128

num_few_shots: 2

jaster:
  message_intro: '以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。'
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/jaster:production'
  dataset_dir: 'jaster'
  jhumaneval:
    # Dify Sandbox configuration for secure code execution
    dify_sandbox:
      endpoint: 'http://dify-sandbox:8194'  # Sandbox service endpoint
      api_key: 'dify-sandbox'               # API key for authentication
      connect_timeout: 10.0                 # Connection timeout in seconds
      read_timeout: 60.0                    # Read timeout in seconds  
      write_timeout: 60.0                   # Write timeout in seconds
      enable_network: true                  # Enable network access in sandbox
      worker_timeout: 15                    # Worker process timeout in seconds
  override_max_tokens: null # if null, use value defined in task dataset files

jbbq:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4-private/jbbq:production'
  dataset_dir: 'jbbq'
  language: 'ja'
  generator_config:
    max_tokens: 10

toxicity:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4-private/toxicity_dataset_full:production'
  judge_prompts_path: 'llm-leaderboard/nejumi-leaderboard4-private/toxicity_judge_prompts:production'
  generator_config:
    max_tokens: 1024
  judge:
    model: 'gpt-4.1-2025-04-14'
    parallel: 32
    params: {} # Currently using OpenAI's default

jtruthfulqa:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4/jtruthfulqa_dataset:production'  # JTruthfulQAデータセットのアーティファクトパス
  roberta_model_name: 'nlp-waseda/roberta_jtruthfulqa'  # 評価に使用するRoBERTaモデル名
  generator_config:
    max_tokens: 256

swebench:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/swebench_verified_official:production'  # SWE-Bench Verifiedデータセットのアーティファクトパス
  dataset_dir: 'swebench_verified_official'
  max_samples: 50  # 全500件は時間がかかるため制限（フルセットの場合は500を指定）
  max_tokens: 2048  # SWE-Benchのパッチ生成に必要なトークン数
  max_workers: 8  # 並列実行ワーカー数
  evaluation_method: 'official'  # 'official' or 'docker' - 公式パッケージまたは独自Docker実装
  background_eval: true # trueの場合は他のベンチマーク実行中にバックグラウンドで評価を行う

mtbench:
  generator_config:
    max_tokens: 1024
    temperature: 0.7
  temperature_override:
    writing: 0.7
    roleplay: 0.7
    extraction: 0.0
    math: 0.0
    coding: 0.0
    reasoning: 0.0
    stem: 0.1
    humanities: 0.1
  question_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_question:production' # if testmode is true, small dataset will be used
  question_artifacts_path_test: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_question_small_for_test:production'
  referenceanswer_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_referenceanswer:production' # if testmode is true, small dataset will be used
  referenceanswer_artifacts_path_test: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_referenceanswer_small_for_test:production'
  judge_prompt_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_prompt:production' 
  bench_name: 'japanese_mt_bench'
  model_id: null # cannot use '<', '>', ':', ''', '/', '\\', '|', '?', '*', '.'
  first_n: null
  judge:
    model: 'gpt-4.1-2025-04-14'
    parallel: 32
    params:
      max_tokens: 2048
      temperature: 0.0

bfcl:
  temperature: 0.01
  num_threads: 4
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/bfcl:production'

hallulens:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/hallulens:production'
  generator_config:
    max_tokens: 256
    temperature: 0.0
    top_p: 1.0
  judge:
    model: 'gpt-4.1-2025-04-14'
    parallel: 32
    params: {} # Currently using OpenAI's default

hle:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4/hle-ja:production'
  dataset_name: 'hle-ja.jsonl'
  max_samples: 10
  generator_config:
    max_tokens: 4096
  judge:
    model: "o3-mini-2025-01-31"
    parallel: 32
    params: {} # Currently using OpenAI's default

arc_agi_2:
  artifacts_path: "llm-leaderboard/nejumi-leaderboard4/arc-agi-2_public-eval_50:production"
  max_output_tokens: 4096
  num_attempts: 2

m_ifeval:
  artifacts_path: "llm-leaderboard/nejumi-leaderboard4/m_ifeval:production"
  dataset_dir: "ja_input_data.jsonl"

sample_dataset:
  artifacts_path: "your artifact path here"
  # add necessary configration here
