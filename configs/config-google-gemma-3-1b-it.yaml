wandb:
  run_name: "google/gemma-3-1b-it" # name of model

api: "vllm-docker"  # 最もシンプルな指定（推奨）
# base_urlは自動的に "http://vllm:8000/v1" が設定されます

batch_size: 32

model:
  use_wandb_artifacts: false
  pretrained_model_name_or_path: "google/gemma-3-1b-it" # huggingface model name
  bfcl_model_id: "google/gemma-3-1b-it" # select "united_oss_fc" or "united_oss_jsonschema". If it doesn't work, find id from "llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/SUPPORTED_MODELS.md"
  size: 1000000000  # parameter size
  size_category: "Small (<10B)" # "Small (<10B)", "Medium (10–30B)", "Large (30B+)"
  base_model: "gemma-3" # base model name such as "llama3.1", "qwen2.5"
  release_date: 3/12/2025

generator:
  # vLLM側の最大文脈長(20000)に収まるよう縮小
  max_tokens: 1024 
  temperature: 0.1
  top_p: 1

# vLLM server configuration
vllm:
  vllm_tag: latest
  lifecycle: auto
  # chat-template: # examples/tool_chat_template_llama3.1_json.jinja
  # 以下はモデルにより調整が必要な場合のみ設定。
  # disable_triton_mma: false
  # dtype: bfloat16
  max_model_len: 12000
  # device_map: auto
  gpu_memory_utilization: 0.90
  # reasoning_parser: deepseek_r1
  # ↓ Tool calling を有効にする場合は以下を設定。bfcl_model_idで"united_oss_fc"を利用する際は必要
  # enable_auto_tool_choice: true
  # tool_call_parser: hermes # variable can be found in https://docs.vllm.ai/en/v0.7.3/features/tool_calling.html#automatic-function-calling
  trust_remote_code: false

# Number of GPUs for tensor parallelism
num_gpus: 1 
