wandb:
  run_name: "Qwen/Qwen3-1.7B" # name of model

api: "vllm-docker"  # 最もシンプルな指定（推奨）
# base_urlは自動的に "http://vllm:8000/v1" が設定されます

batch_size: 32

model:
  use_wandb_artifacts: false
  pretrained_model_name_or_path: "Qwen/Qwen3-1.7B" # huggingface model name
  bfcl_model_id: "Qwen/Qwen3-1.7B-FC" # select "united_oss_fc" or "united_oss_jsonschema". If it doesn't work, find id from "llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/SUPPORTED_MODELS.md"
  size: 1700000000 # parameter size
  size_category: "Small (<10B)" # "Small (<10B)", "Medium (10–30B)", "Large (30B+)"
  base_model: "Qwen3" # base model name such as "llama3.1", "qwen2.5"
  release_date:  # 4/29/2025

generator:
  extra_body:
    chat_template_kwargs:
        enable_thinking: false
  max_tokens: 24000 
  temperature: 0.1
  top_p: 1

# vLLM server configuration
vllm:
  vllm_tag: latest
  lifecycle: auto
  # chat-template: # examples/tool_chat_template_llama3.1_json.jinja
  # 以下はモデルにより調整が必要な場合のみ設定。
  # disable_triton_mma: false
  # dtype: bfloat16
  # max_model_len: 8192
  # device_map: auto
  gpu_memory_utilization: 0.8
  # reasoning_parser: deepseek_r1
  # ↓ Tool calling を有効にする場合は以下を設定。bfcl_model_idで"united_oss_fc"を利用する際は必要
  # enable_auto_tool_choice: true
  # tool_call_parser: hermes # variable can be found in https://docs.vllm.ai/en/v0.7.3/features/tool_calling.html#automatic-function-calling
  # trust_remote_code: false

# Number of GPUs for tensor parallelism
num_gpus: 1 

