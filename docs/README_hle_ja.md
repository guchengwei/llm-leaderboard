# HLE-JA 評価ガイド

**TL;DR**

*   **何を測る？**
    現行のAIモデルでは解答が困難な、AIの能力の限界を試すために設計されたベンチマーク。
*   **評価セット**
    "Humanity's Last Exam" (HLE) の日本語版。`hle-ja.csv`をソースとし、カテゴリに基づき層化分割（8:1:1）されている。
*   **出力形式**
    モデルは「説明」「答え」「信頼度」の3つの要素を含むテキストを生成するよう求められる。

---

## 1. HLE-JA とは

*   **概要**: HLE-JAは、Center for AI SafetyとScale AIによって作成されたベンチマーク "Humanity's Last Exam" (HLE) の日本語版です。MMLUのような既存のベンチマークが高性能なLLMにとって簡単になりすぎたため、より挑戦的な評価として開発されました。
*   **特徴**: 問題は、単純なWeb検索では答えられないように意図的に設計されています。オリジナルのデータセットは、多くの分野にわたる2,500の質問を含み、そのうち約14%はマルチモーダル（テキストと画像）です。トップモデルでも現在のスコアは3〜14%と低く、その難易度の高さを示しています。

---

## 2. 評価セットの作り方

*   **目的**: HLEのフレームワークに基づき、日本語の言語モデル向けの挑戦的なベンチマークを作成する。
*   **ソースファイル**: `hle-ja.csv`
*   **処理**:
    1.  `category`列に基づき、データを層化サンプリング。
    2.  訓練 (80%)、開発 (10%)、テスト (10%) の割合で3つに分割。
    3.  それぞれ `train.jsonl`, `dev.jsonl`, `test.jsonl` として保存。
*   **配布形態**: W&B アーティファクト (`wandb.Artifact`)
    *   **名前**: `hle-ja` （バージョンで管理）
    *   **アップロードスクリプト**: `scripts/data_uploader/upload_hle.py`

---

## 3. 問題カテゴリの内訳

オリジナルのHLEデータセットは、以下のカテゴリで構成されています。`hle-ja`でも、このカテゴリ比率が維持されるようにサンプリングされています。

| カテゴリ                   | 問題数 |
| -------------------------- | ------:|
| Biology/Medicine           |    280 |
| Chemistry                  |    165 |
| Computer Science/AI        |    241 |
| Engineering                |    111 |
| Humanities/Social Science  |    219 |
| Math                       |  1,021 |
| Other                      |    233 |
| Physics                    |    230 |
| **合計**                   | **2500** |

---

## 4. オリジナルHLEと日本語版の比較

`hle-ja`は、オリジナルのHLEをベースに、日本のLLMを評価するために調整されたバージョンです。主な違いは以下の通りです。

| 項目           | 詳細                                                                                             |
| -------------- | ------------------------------------------------------------------------------------------------ |
| **翻訳**       | 全ての問題文と選択肢は、`qwen/qwen3-235b-a22b`モデルを用いて日本語に機械翻訳されています。         |
| **内容の変更** | オリジナルに含まれる画像付きのマルチモーダル問題は、今回の評価対象から**除外**されています。       |
| **サンプリング** | 画像を含まないテキストベースの問題群から、オリジナルのカテゴリ比率を維持するようにサンプリングされています。 |

---

## 5. 評価フロー（全体像）

1.  **データセット取得**: W&B Artifactsから指定されたバージョンの `hle-ja` データセットをダウンロードします。
2.  **回答生成**: `dev` または `test` セットの各質問に対し、評価対象モデルが回答を生成します。プロンプトには、「説明」「答え」「信頼度」を含む特定の形式で出力するよう指示が含まれます。
3.  **正誤判定 (Judge)**: 生成された回答は、独立したジャッジモデル（例: `o3-mini-2025-01-31`）によって評価されます。ジャッジモデルは、モデルの回答と事前に用意された正解を比較し、`correct: "yes" or "no"` を含む判定結果を構造化データとして出力します。
4.  **メトリクス計算**: 全ての回答の判定結果を元に、正解率 (`accuracy`) やキャリブレーションエラー (`calibration_error`) などの最終的なスコアを計算します。
5.  **結果のログ**: 最終スコア、および各質問の詳細な結果（生成された回答、判定理由など）がW&Bのテーブルとして記録されます。

---

## 6. 入出力仕様（実例）

### 入力 (データセット)

*   **形式**: JSONL
*   **主要フィールド**: `question` (質問文), `answer` (正解), `category` など。
*   **備考**: 質問に画像が含まれる場合もありますが、評価スクリプトは画像に対応していないモデルに対して警告を出します。

### 出力 (評価対象モデル)

```text
説明: {選択した答えに対する説明}
答え: {選択した答え}
信頼度: {答えに対する0%から100%の信頼度スコア}
```

### 出力 (ジャッジモデル)

ジャッジモデルは、評価対象モデルの回答を解析し、以下の情報を含むJSONを出力します。

```json
{
  "extracted_final_answer": "抽出された最終回答",
  "reasoning": "正解と比較した判定理由",
  "correct": "yes" or "no",
  "confidence": "抽出された信頼度スコア"
}
```

---

## 7. 実行方式と環境

*   **評価スクリプト**: `scripts/evaluator/hle.py` 内の `evaluate()` 関数がエントリーポイントです。
*   **非同期処理**: 評価は `asyncio` を用いて非同期で実行され、モデルへのリクエストやジャッジ処理を並列化しています。
*   **推論制御**: `generator.*` で温度やトークン数を調整します。

---

## 8. コンフィグ主要項目

設定はYAMLファイルで管理され、`hle` キーの下に以下の項目があります。

```yaml
hle:
  artifact_path: "path/to/your/wandb_artifact:version"  # 評価に使うデータセットのArtifactパス
  dataset_name: 'hle-ja.jsonl'                          # 評価に使うデータセットのファイル名
  max_samples: 10                                      # 評価に使う最大サンプル数
  judge:
    model: "o3-mini-2025-01-31"                       # 判定に使用するジャッジモデル
    parallel: 32                                      # ジャッジ処理の並列数
  generator_config:                                   # 回答生成時のモデルパラメータ
    max_tokens: 4096
```

---

## 9. FAQ

*   **Q. 正解かどうはどのように判定されますか？**
    → 評価対象モデルの回答を、独立した「ジャッジモデル」が正解と比較して `yes` か `no` で判定します。これにより、表記揺れなどを吸収しつつ、より人間に近い判断を目指しています。

*   **Q. `calibration_error` とは何ですか？**
    → モデルが回答と共に出力する「信頼度」が、実際の正解率とどれだけ一致しているかを示す指標です。この値が低いほど、モデルが自身の確信度を正しく予測できていることを意味します。

